# ğŸ§  LLM Microservice Hub

A production-grade **FastAPI + AsyncIO** microservice designed for Large Language Model (LLM) integrations.  
This service provides API endpoints for **chat completions**, **embeddings**, and includes an extendable structure for future AI workflows such as async pipelines, rate-limited LLM calls, and background processing.

This project demonstrates clean backend engineering practices, modular design, and async-first architectures used in modern AI applications.

---

## ğŸš€ Features

- **FastAPI-based architecture** with modular routing  
- **LLM chat endpoint** (`/chat`) for conversational responses  
- **Embeddings endpoint** (`/embeddings`) returning vector encodings  
- **AsyncIO-powered architecture** for non-blocking performance  
- **Service-layer wrappers** prepared for OpenAI or any LLM provider  
- **Clean folder structure** for scalability  
- **Pydantic validation models** for clear API contracts  
- Ready for future additions: caching, retries, rate limiting, and model routing  

---

## ğŸ“‚ Project Structure

```
llm-microservice-hub/
â”‚
â”œâ”€â”€ app/                     # FastAPI entrypoint, router mounting
â”‚   â”œâ”€â”€ main.py
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py          # Chat completion endpoint
â”‚   â”‚   â””â”€â”€ embeddings.py    # Embeddings endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # Shared Pydantic schemas
â”‚   â”‚
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ llm_client.py    # Placeholder LLM service client
â”‚
â””â”€â”€ README.md
```

---

## â–¶ï¸ Run Locally

```bash
uvicorn app.main:app --reload
```

---

## ğŸ”Œ API Endpoints

---

### **1. POST /chat**

#### Request  
```json
{
  "message": "Hello!"
}
```

#### Example Response  
```json
{
  "reply": "Hi! How can I assist you today?"
}
```

---

### **2. POST /embeddings**

#### Request  
```json
{
  "text": "Machine learning is powerful."
}
```

#### Example Response  
```json
{
  "embedding": [0.123, 0.552, 0.982, 0.441]
}
```

---

## ğŸ§© Extending With a Real LLM Provider

Modify `services/llm_client.py` to integrate:

- **OpenAI API**
- **Anthropic Claude**
- **Groq LLM**
- **Custom / self-hosted models**

The architecture is intentionally designed so developers can easily replace the placeholder implementation with a real LLM backend.

---

## ğŸ›  Tech Stack

- Python 3.10+  
- FastAPI  
- AsyncIO  
- Uvicorn  
- Pydantic  

---

## ğŸ“Œ Status

This project is part of an LLM engineering portfolio demonstrating production-grade backend practices, async patterns, and AI integration workflows.
